{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Course Project**\n",
        "### Course Intructor: Prof: Fabrizio Silvestri \n",
        "## **Topic: Image Super Resolution**\n",
        "### Project Members:\n",
        "### Saud Hussain (1990559)\n",
        "### Faisal Gul (1950572)\n",
        "### Rida Amjad (1684582)"
      ],
      "metadata": {
        "id": "oABuEAia1HGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We have choosen the task of Image Super Resolution in this Deep Learning Course. \n",
        "## We have used the BSDS500 Dataset for the training and validation purposes.\n",
        "## We have used Set5 Dataset for the testing purposes.\n",
        "## We have implemented 4 different interpolation methods to convert images into low resolutions and then rescontruct them. \n",
        "## In addition to this for each interpolation method we are scaling the images by 2x and 4x factors."
      ],
      "metadata": {
        "id": "sp6GYsqK2EXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MOUNTING THE CONTENT FROM DRIVE"
      ],
      "metadata": {
        "id": "zUf2qkodgGKy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "aSlC_TSMoBQE",
        "outputId": "558961c7-3107-4ccb-bc59-a2b9ab94f050"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTING ALL THE NECESSARY LIBRARIES"
      ],
      "metadata": {
        "id": "7IFeFvRhgV9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input, regularizers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import keras"
      ],
      "metadata": {
        "id": "MX2HOP4PoIAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING THE TRAINING AND VALIDATION DATA.\n",
        "\n",
        "\n",
        "## We have used BSDS500 Dataset for the training and validation purpose.\n",
        "\n",
        "## The dataset BSDS500 has 200 images for training, 100 for validation.\n",
        "\n",
        "## We also normalize the images during this phase."
      ],
      "metadata": {
        "id": "VyG5pMrfgcN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"drive/MyDrive/ISRDataset/BSDS500/data/\""
      ],
      "metadata": {
        "id": "2oQSPNUWoLxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "folder='images'\n",
        "\n",
        "train=[]\n",
        "val=[]\n",
        "test=[]\n",
        "\n",
        "for fold in os.listdir(path+folder):\n",
        "  arr=[]\n",
        "  \n",
        "  with tqdm(total=len(os.listdir(path+folder+\"/\"+fold))) as pbar:\n",
        "    for img in os.listdir(path+folder+\"/\"+fold):\n",
        "      im=cv2.imread(path+folder+\"/\"+fold+\"/\"+img,0)\n",
        "      try:\n",
        "        im=cv2.resize(im,(80,80))\n",
        "        im=im/255.0\n",
        "        arr.append(im)\n",
        "        pbar.update(1)\n",
        "      except Exception as e:\n",
        "        print(\"EXC: \", e)\n",
        "        pass\n",
        "    if fold=='train':\n",
        "      train=np.array(arr)\n",
        "    elif fold=='val':\n",
        "      val=np.array(arr)\n",
        "    elif fold=='Set5':\n",
        "      test=np.array(arr)\n",
        "    \n",
        "\n",
        "train.shape"
      ],
      "metadata": {
        "id": "_XzLUEIooOwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train=np.expand_dims(train,axis=3)\n",
        "#train=np.squeeze(train,axis=3)\n",
        "train.shape\n",
        "train=np.expand_dims(train,axis=3)\n",
        "val=np.expand_dims(val,axis=3)\n",
        "train.shape"
      ],
      "metadata": {
        "id": "02Q-qvJqupCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Through our this function we are lowering the resolution of our images using interpolation methods of the cv2 Library.\n",
        "## We resize images into 80x80 size.\n",
        "\n",
        "## We use the scalingfactor of 2x\n",
        "\n",
        "#### This function is inspired from the following code with modifications made by us (https://towardsdatascience.com/image-super-resolution-using-convolution-neural-networks-and-auto-encoders-28c9eceadf90)"
      ],
      "metadata": {
        "id": "jwqGQl9-g4sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low_resol_img_2x_inter_area(img, scaling = 2):\n",
        "  w = int(img.shape[1] //2)\n",
        "  h = int(img.shape[0] //2)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_AREA )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 2)\n",
        "  h = int(lowresolution_img.shape[0] * 2)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_AREA)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "KPvgc8Wwq0Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape\n",
        "val.shape\n"
      ],
      "metadata": {
        "id": "HOHhHMbItioD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The low_train and low_valid contain the low resolution images. "
      ],
      "metadata": {
        "id": "W-kFpVAehn8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_2x_inter_area(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_2x_inter_area(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "LiaSjWRFtmK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "S9SbNjQpvOi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We use the autoencoder model to train on our training images and validate on our validation images.\n",
        "\n",
        "### This autoencoder model is inspired from the following code with little modification (https://towardsdatascience.com/image-super-resolution-using-convolution-neural-networks-and-auto-encoders-28c9eceadf90)"
      ],
      "metadata": {
        "id": "NOjmXoxrh7iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input_img = Input(shape=(80, 80,1))  \n",
        "    \n",
        "#encoding architecture\n",
        "x1 = Conv2D(64, (3, 3),kernel_initializer=\"glorot_uniform\", activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(Input_img)\n",
        "x2 = Conv2D(64, (3, 3), kernel_initializer=\"glorot_uniform\",activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x1)\n",
        "x3 = MaxPool2D(padding='same')(x2)\n",
        "x4 = Conv2D(128, (3, 3), kernel_initializer=\"glorot_uniform\",activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x3)\n",
        "x5 = Conv2D(128, (3, 3), kernel_initializer=\"glorot_uniform\",activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x4)\n",
        "x6 = MaxPool2D(padding='same')(x5)\n",
        "encoded = Conv2D(256, (3, 3), kernel_initializer=\"glorot_uniform\", activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x6)\n",
        "# decoding architecture\n",
        "x7 = UpSampling2D()(encoded)\n",
        "x8 = Conv2D(128, (3, 3), kernel_initializer=\"glorot_uniform\", activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x7)\n",
        "x9 = Conv2D(128, (3, 3), kernel_initializer=\"glorot_uniform\", activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x8)\n",
        "x10 = Add()([x5, x9])\n",
        "x11 = UpSampling2D()(x10)\n",
        "x12 = Conv2D(64, (3, 3),kernel_initializer=\"glorot_uniform\",  activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x11)\n",
        "x13 = Conv2D(64, (3, 3), kernel_initializer=\"glorot_uniform\", activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x12)\n",
        "x14 = Add()([x2, x13])\n",
        "\n",
        "decoded = Conv2D(3, (3, 3),kernel_initializer=\"glorot_uniform\",  padding='same',activation='relu', kernel_regularizer=regularizers.l1(10e-10))(x14)\n",
        "autoencoder = Model(Input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "1FD_H1gattm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')"
      ],
      "metadata": {
        "id": "bYzjVXIHuJxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "Z_PwzpX3uP0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_score = autoencoder.evaluate(low_valid, val)\n"
      ],
      "metadata": {
        "id": "-aP2cL2V0CfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The validation loss with interpolation method (Inter Area) and Scaling Factor (2x) is 0.005"
      ],
      "metadata": {
        "id": "YlNkIm5djOTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('validation Loss', loss_score) "
      ],
      "metadata": {
        "id": "7SuYQU6AT6TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The validation Accuracy with interpolation method (Inter Area) and Scaling Factor (2x) is 99.47"
      ],
      "metadata": {
        "id": "7WKNmAcSjZnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('validation accuracy', (1-loss_score)*100)"
      ],
      "metadata": {
        "id": "7Z9eNE8qT9jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the training and validation loss"
      ],
      "metadata": {
        "id": "61GTGaAXjgWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WV2PcQOL08I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the test dataset.\n",
        "\n",
        "## We use the SET5 Dataset for testing purpose.\n",
        "\n",
        "## Set5 dataset consists of 5 images."
      ],
      "metadata": {
        "id": "QN2mO6yejk0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "folder='Set5/'\n",
        "\n",
        "test=[]\n",
        "arr=[]\n",
        "names=[]\n",
        "with tqdm(total=len(os.listdir(path+\"images/\"+folder))) as pbar:\n",
        "  for img in os.listdir(path+\"images/\"+folder):\n",
        "    names.append(img)\n",
        "    im=cv2.imread(path+\"images/\"+folder+\"/\"+img,0)\n",
        "    try:\n",
        "      im=cv2.resize(im,(80,80))\n",
        "      im=im/255.0\n",
        "      arr.append(im)\n",
        "      pbar.update(1)\n",
        "    except Exception as e:\n",
        "      print(\"EXC: \", e)\n",
        "      pass\n",
        "    test=np.array(arr)\n",
        "    pbar.update(1)\n",
        "\n",
        "test.shape"
      ],
      "metadata": {
        "id": "IzREjzpk1Y4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=np.expand_dims(test,axis=3)"
      ],
      "metadata": {
        "id": "tO496q1g0IG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passing the test images through low resolutin function to lower the testing images resolution"
      ],
      "metadata": {
        "id": "iz1dxG5Vj0VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_2x_inter_area(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "pGK9pGhJ0ge0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)\n"
      ],
      "metadata": {
        "id": "4bqztK_M0m_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting the outputs (low resolution-->high resolution conversion)"
      ],
      "metadata": {
        "id": "7XfTd7Vij-FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "mlrjICWK0up3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We calculate the Mean Squared Error of the High Resolution and Our Reconstructed Images.\n",
        "## AND\n",
        "## Mean Squared Error of the High Resolution Images and Degraded Images.\n",
        "\n",
        "## If the MSE decreases, this means our model is working well."
      ],
      "metadata": {
        "id": "eJ7Rjvkmka43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "HGuPmVN80zTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.shape"
      ],
      "metadata": {
        "id": "7PiV20WY4xK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Degraded, Reconstructed and Original High Resolution Images."
      ],
      "metadata": {
        "id": "mLcZ8qgEk4AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[2].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TDujR6vl4QnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[2])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "462XyLZW4YoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[2].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R5sKUw_o4hDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We define function to calculate the PNSR "
      ],
      "metadata": {
        "id": "CrWoHGDdlC6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PSNR(y_true,y_pred):\n",
        "    mse=tf.reduce_mean( (y_true - y_pred) ** 2)\n",
        "    return 20 * log10(1 / (mse ** 0.5))\n",
        "\n",
        "def log10(x):\n",
        "    numerator = tf.math.log(x)\n",
        "    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
        "    return numerator / denominator"
      ],
      "metadata": {
        "id": "B0-GAeo3Amr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We calculate the PNSR of the Degraded and High Resolution and Compare it with the Reconstructed and High Resolution Images.\n",
        "\n",
        "## If the PNSR of Reconstructed Images if greater then the Degraded, this means our model is providing good results."
      ],
      "metadata": {
        "id": "hXRnbGphlHPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "3JxIOxkEAn5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and keeping the interpolation function same as Inter Area but change the scaling factor to 4x\n",
        "\n",
        "\n",
        "###### NOTE: Reload the training and validation, testing data again and expand their dimensions and then run the following cells. "
      ],
      "metadata": {
        "id": "F3CDoVyHll0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def low_resol_img_4x_inter_area(img, scaling = 4):\n",
        "  w = int(img.shape[1] //4)\n",
        "  h = int(img.shape[0] //4)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_AREA )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 4)\n",
        "  h = int(lowresolution_img.shape[0] * 4)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_AREA)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "DEKjvqdQlZvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_4x_inter_area(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_4x_inter_area(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "H5KikZ99Mvoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "OKRPyiW3MwmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "7e4lBxSxNBYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_4x_inter_area(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "1siObtEfNM18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "VimuCp_HNUqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "z4yoxnblNVqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE with interpolation Inter Area and Scaling 4x "
      ],
      "metadata": {
        "id": "sh6XIlJSmQsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "FXDaSyatNbAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "42G_3HNSmfns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[1].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eIz47qx9NgGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[1])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pX3vnOMqNkzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[1].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mX0Py6uaNlbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR after interpolation is Inter Area and Scaling is 4x"
      ],
      "metadata": {
        "id": "qzNzsUFLmo4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "F9rKlan8Ny35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and change the **interpolation function to Inter Nearest** and change the **scaling factor to 2x**\n",
        "##### NOTE: Reload the training and validation, testing data again and expand their dimensions and then run the following cells."
      ],
      "metadata": {
        "id": "WoYGg66nnIfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def low_resol_img_2x_inter_nearest(img, scaling = 2):\n",
        "  w = int(img.shape[1] //2)\n",
        "  h = int(img.shape[0] //2)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_NEAREST )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 2)\n",
        "  h = int(lowresolution_img.shape[0] * 2)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_NEAREST)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "QC_QMCNAlsJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_2x_inter_nearest(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_2x_inter_nearest(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "CfOYD-DXPGho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "8aFlA2yEPGqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "luFeycXOPGyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_2x_inter_nearest(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "8NNYkaOtPG69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "DU-0MaUnPHEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "mP1n_2KwPHMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE with interpolation Inter Nearest and Scaling 2x"
      ],
      "metadata": {
        "id": "bxSnGZsXniBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "9PRc04Z2PHPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "AKxJeU6ynrhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[2].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xgBi_F0uPHeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[2])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uJ91QReWPHlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[2].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b7OAh2_hPHsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR after interpolation is Inter Nearest and Scaling is 2x"
      ],
      "metadata": {
        "id": "HI-CVXtjn1Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "dPkd8PLIPUm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and keeping the **interpolation function same as Inter Nearest** but change the **scaling factor to 4x**\n",
        "##### NOTE: Reload the training and validation, testing data again and expand their dimensions and then run the following cells."
      ],
      "metadata": {
        "id": "h9TM_3qYoBOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low_resol_img_4x_inter_nearest(img, scaling = 4):\n",
        "  w = int(img.shape[1] //4)\n",
        "  h = int(img.shape[0] //4)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_NEAREST )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 4)\n",
        "  h = int(lowresolution_img.shape[0] * 4)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_NEAREST)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "ofvEWjYkls02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_4x_inter_nearest(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_4x_inter_nearest(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "bG2bRuf7PgCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "8UGOyK0ePgLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "bF0ubTLuPgTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_4x_inter_nearest(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "rhWgF0NHPgbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "F__nyqHFPgj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "NCix6PzePgrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE after interpolation is Inter Nearest and Scaling is 4x"
      ],
      "metadata": {
        "id": "yrO_MXTPoWF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "RMJ3aRtlPgzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[1].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ts3722QcPg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[1])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OoZXO1dvPhDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[1].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CUdxyel4PhLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR after interpolation is Inter Nearest and Scaling is 4x"
      ],
      "metadata": {
        "id": "j9eau0f2oe9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "Y4wxk9fHPhTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and change the interpolation function to Inter Cubic and change the scaling factor to 2x\n",
        "##### NOTE: Reload the training and validation, testing data again and expand their dimensions and then run the following cells."
      ],
      "metadata": {
        "id": "u4nrYyIiovPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low_resol_img_2x_inter_cubic(img, scaling = 2):\n",
        "  w = int(img.shape[1] //2)\n",
        "  h = int(img.shape[0] //2)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_CUBIC )\n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 2)\n",
        "  h = int(lowresolution_img.shape[0] * 2)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_CUBIC)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "49vw3buBKUqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_2x_inter_cubic(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_2x_inter_cubic(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "w8sFtngnR2NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "UP8yaszxR2V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "x8p7uk6wR2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_2x_inter_cubic(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "NhfSENKMR2yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "30fS8t12R250"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "OspGys3QR3Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE with interpolation Inter Cubic and Scaling 2x"
      ],
      "metadata": {
        "id": "kEOXdUF0pFoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "bcQeK8D3R3N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "t6SjTApJpJZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[4].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wPxkxOmAR3VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[4])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZGRaa-BCR3b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[4].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_5Y6oCuZR3o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR with interpolation Inter Cubic and Scaling 2x"
      ],
      "metadata": {
        "id": "iIx4AMXxpSV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "Z72t-Xh3R3vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and keeping the interpolation function same as Inter Cubic but change the scaling factor to 4x\n",
        "##### NOTE: Reload the training and validation, testing data again and expand their dimensions and then run the following cells"
      ],
      "metadata": {
        "id": "JxCjvBiPpZNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low_resol_img_4x_inter_cubic(img, scaling = 4):\n",
        "  w = int(img.shape[1] //4)\n",
        "  h = int(img.shape[0] //4)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_CUBIC )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 4)\n",
        "  h = int(lowresolution_img.shape[0] * 4)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_CUBIC)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "UU5R927YKVL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_4x_inter_cubic(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_4x_inter_cubic(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "ZTCW5pNbSH6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "NI1VUaiSSIBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "JbF3Lj86SIIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_4x_inter_cubic(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "YZQDCg3SSIQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "JpXM61ZcSIXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "1TqY5xSNSIeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE with interpolation Inter Cubic and Scaling 4x"
      ],
      "metadata": {
        "id": "aXlcharjppcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "MuNALgf0SIlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "vgayBBECp19H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[4].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tWxO_RQkSIrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[4])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y6jIeDdoSIyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[4].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kducSz7VSI4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR after interpolation is Inter Cubic and Scaling is 4x"
      ],
      "metadata": {
        "id": "1SVex4ZCp73q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "8KPFMe2ZSI_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and **change the interpolation function to Inter Linear and change the scaling factor to 2x**\n",
        "##### NOTE: Reload the training and validation data again and expand their dimensions and then run the following cells."
      ],
      "metadata": {
        "id": "_itLqzIOqE3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low_resol_img_2x_inter_linear(img, scaling = 2):\n",
        "  w = int(img.shape[1] //2)\n",
        "  h = int(img.shape[0] //2)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_LINEAR )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 2)\n",
        "  h = int(lowresolution_img.shape[0] * 2)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_LINEAR)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "1W3lqro4KWLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_2x_inter_linear(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_2x_inter_linear(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "i6dhn4f6S2cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "n3XZoHOWS2ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "v9-5XN7vS2uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_2x_inter_linear(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "4i9If35iS3AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "2j86Fc_LS3HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "UrIHcDl2S3N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE with interpolation Inter Linear and Scaling 2x"
      ],
      "metadata": {
        "id": "6Kjo65_tqaCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "mdmMKq_GS3Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "u8c8Z8rgqkTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[1].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BGmjQqR4S3d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[1])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HXlpoI8-S3jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[1].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pgDgEFQRS4R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR with interpolation Inter Linear and Scaling 2x"
      ],
      "metadata": {
        "id": "03OoiDSgqq7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "Wmt9gpK6S4aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we define a new function and keeping the interpolation function **same as Inter Linear but change the scaling factor to 4x**\n",
        "##### NOTE: Reload the training and validation, testing data again and expand their dimensions and then run the following cells"
      ],
      "metadata": {
        "id": "pVPCZ6Jwq04K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def low_resol_img_4x_inter_linear(img, scaling = 4):\n",
        "  w = int(img.shape[1] //4)\n",
        "  h = int(img.shape[0] //4)\n",
        "  dimensions = (w, h)\n",
        "  lowresolution_img = cv2.resize(img, dimensions, interpolation = cv2.INTER_LINEAR )\n",
        "  \n",
        "  \n",
        "  w = int(lowresolution_img.shape[1] * 4)\n",
        "  h = int(lowresolution_img.shape[0] * 4)\n",
        "  dimensions = (w, h)\n",
        "  lowres_img = cv2.resize(lowresolution_img, dimensions, interpolation =  cv2.INTER_LINEAR)\n",
        "  return lowres_img"
      ],
      "metadata": {
        "id": "eqOgm1MvKWU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train = []\n",
        "for i in range(train.shape[0]):\n",
        "  x = low_resol_img_4x_inter_linear(train[i,:,:,:])\n",
        "  low_train.append(x)\n",
        "low_train = np.array(low_train)   \n",
        "\n",
        "\n",
        "low_valid = []\n",
        "for i in range(val.shape[0]):\n",
        "  x = low_resol_img_4x_inter_linear(val[i,:,:,:])\n",
        "  low_valid.append(x)\n",
        "low_valid = np.array(low_valid)     \n",
        "\n",
        "low_train.shape"
      ],
      "metadata": {
        "id": "NKqEq9SXTIaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_train=np.expand_dims(low_train,axis=3)\n",
        "low_valid=np.expand_dims(low_valid,axis=3)"
      ],
      "metadata": {
        "id": "JrIj24yBTIhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(low_train,train,\n",
        "            epochs=100,\n",
        "            validation_data=(low_valid, val),\n",
        "            callbacks=[early_stopper]\n",
        "            )"
      ],
      "metadata": {
        "id": "IpWTUNmfTIo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test = []\n",
        "for i in range(test.shape[0]):\n",
        "  x = low_resol_img_4x_inter_linear(test[i,:,:,:])\n",
        "  low_test.append(x)\n",
        "low_test = np.array(low_test)   \n",
        "low_test.shape"
      ],
      "metadata": {
        "id": "wXTeYIZlTIwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_test=np.expand_dims(low_test,axis=3)"
      ],
      "metadata": {
        "id": "XwYOuMTQTI4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=autoencoder.predict(low_test)"
      ],
      "metadata": {
        "id": "s5bC6xyYTI_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## MSE with interpolation Inter Linear and Scaling 4x"
      ],
      "metadata": {
        "id": "BRTEbJuTrKVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"MSE of High Resolution and Our Reconstructed Image\")\n",
        "  print(names[i],mse(test[i], pred).numpy())\n",
        "  print(\"MSE of High Resolution and Degraded Image\")\n",
        "  print(names[i],mse(low_test[i], test[i]).numpy())\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "9LE_rx8RTJGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "nLFbHfbdrO3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(low_test[2].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Degraded\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bAByo-bSTJNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(preds[2])\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nmg7hNLXTJVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test[2].reshape(80,80),cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vq3Q2fAgTJjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PNSR after interpolation is Inter Linear and Scaling is 4x"
      ],
      "metadata": {
        "id": "OXwOmalcrV9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "for i,pred in enumerate(preds):\n",
        "  print(\"PNSR of Degraded and High Resolution\")\n",
        "  print(names[i],PSNR(low_test[i],test[i]))\n",
        "  print(\"PNSR of Reconstructed and High Resolution\")\n",
        "  print(names[i],PSNR(pred,test[i]))\n",
        "  print(\"-------------\")"
      ],
      "metadata": {
        "id": "5tqNx3LNTJqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}